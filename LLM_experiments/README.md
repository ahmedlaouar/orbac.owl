# 🧪 LLM_experiments/

This module runs and analyzes experiments on natural language explanations generated by LLMs over ORBAC policies. It builds on the core classes: `Policy`, `AccessRight`, `Explainer`, and `Evaluator`.

---

## 🚀 Running Experiments

Use the CLI interface to launch experiments:
```bash
python LLM_experiments/run_exps.py [zero_shot|few_shot|parameters_tuning|interactive] [model_name]
```

### Arguments:
- `experiment` : Type of experiment (`zero_shot`, `few_shot`, `parameters_tuning`, or `interactive`)
- `model`: Name of the LLM to use (must be in `models_list`)

Logs are saved to `LLM_experiments/tmp/`.

### 🧩 Experiment Types

- Zero-shot / Interactive: Run `explainer.interactive_prompting()` on every access rule (1 vs multiple iterations).

- Few-shot: Run `explainer.few_shot_prompting()` with manually constructed examples.

- Parameters Tuning: Optimize explanation generation parameters using `optuna` (via `explainer.optimize_model()`), store results and metrics in `tmp/best_trial.txt`.

### 📊 Analysis
Use the notebook:

```bash
LLM_experiments/OrBAC_LLMs_stats.ipynb
```
to visualize and analyze results.

### 📁 Folder Structure

- `run_exps.py` – Main script to run all experiments.

- `results/` – CSVs and evaluation outputs.

  - `figures/` – Plots and figures generated by the notebook.

- `tmp/` – Logs and best parameter trials in JSON and .txt.

- `requirements.txt` – contains the needed libraries to run the experiments.